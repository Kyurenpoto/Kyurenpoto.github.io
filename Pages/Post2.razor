@page "/post/2"

<MetaTags title="논문 리뷰: Towards Learning Convolutions from Scratch - " description="논문 리뷰: Towards Learning Convolutions from Scratch" route="/post/2" keywords="deep learning, activation function, skip connection, inverse problem" pathImage="assets/images/sine-curve.png" />

<Link Href="styles/post.css" Rel="stylesheet" />

<div class="content-wrap">
    <h1>논문 리뷰: Towards Learning Convolutions from Scratch</h1>
    
    <p>논문 링크: <a href="https://arxiv.org/abs/2007.13657">https://arxiv.org/abs/2007.13657</a></p>
    <p>논문의 초록, 서론, 결론을 종합하면 다음과 같다.</p>
    <ol>
        <li>최신 NN 아키텍쳐 검색 알고리즘에서는 Convolution을 기본 모듈로 사용한다.</li>
        <li>Fully-Connected NN으로부터 CNN을 유도하기 위해 MDL을 사용했다.</li>
        <li>FCN이 더 잘 학습할 수 있도록 LASSO의 변형인 LASSO<katex-expression expression='-\beta' katex-options='{ "displayMode": false , "throwOnError": true }' />를 사용했다.</li>
    </ol>

    <p>LASSO는 Cost Function에 네트워크 가중치의 L1 Norm을 더해주는 Regularization. Feature Selection 효과.</p>
    <p>Deep-Conv는 FC -> Deep 3x3 Convs, Shallow-Conv는 FC -> Shallow 9x9 Conv</p>
    <p>D-Conv, D-Local, D-FC, S-Conv, S-Local, S-FC, 3-FC에 대한 성능 측정을 통해 얻어낸 직관.</p>
    <ol>
        <li>지역성: Local-FC 차이가 Conv-Local 차이보다 월등. 즉 Convolution의 주요한 이점은 지역성.</li>
        <li>Shallow NN은 결국 Deep NN을 따라잡음: Deep NN은 긴 epoch에서 성능 증가폭이 더디거나 오히려 감소하는 반면, Shallow NN은 일관적으로 성능이 증가.</li>
        <li>Weight sharing이 없으면 Deep NN의 이점이 사라짐: S-FC는 D-FC를 이기고, 긴 epoch에서는 S-Local과 D-Local의 우열을 가리기 힘듬.</li>
        <li>FC의 구조가 문제: 3-FC와 S-FC는 Parameter 수와 Depth가 같지만, S-FC가 성능이 좋음. S-FC는 첫 레이어에서 3-FC에 비해 더 많은 뉴런을 가짐.</li>
    </ol>
    <p>논문의 나머지는 Local-FC 차이를 좁히는 과정.</p>
    <p><a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Shalev-Shwartz and Ben-David, 2014</a>에서 정의한 Minimum Description Language(이하 MDL)<katex-expression expression='L_D\left(h\right)\le L_S\left(h\right)+\sqrt{\frac{\left\lvert d\left(h\right)\right\rvert+\log{2/\delta}}{2m}}' katex-options='{ "displayMode": false , "throwOnError": true }' />을 보면, Generalization-gap은 모델의 파라미터 수로 통제. 그러나 지금까지의 많은 아키텍쳐들을 볼 때, 아키텍쳐를 잘 골라서 파라미터 수를 확장하면 성능이 저하되지 않음. 적은 파라미터로 트레이닝 데이터에 피팅하는 아키텍쳐가 우월함.</p>
</div>