@page "/post/2"

<MetaTags title="논문 리뷰: Towards Learning Convolutions from Scratch - " description="논문 리뷰: Towards Learning Convolutions from Scratch" route="/post/2" keywords="deep learning, activation function, skip connection, inverse problem" pathImage="assets/images/sine-curve.png" />

<Link Href="styles/post.css" Rel="stylesheet" />

<div class="content-wrap">
    <h1>논문 리뷰: Towards Learning Convolutions from Scratch</h1>
    
    <p>논문 링크: <a href="https://arxiv.org/abs/2007.13657">https://arxiv.org/abs/2007.13657</a></p>
    <p>논문의 초록, 서론, 결론을 종합하면 다음과 같다.</p>
    <ol>
        <li>최신 NN 아키텍쳐 검색 알고리즘에서는 Convolution을 기본 모듈로 사용한다. 아키텍쳐 검색 알고리즘처럼 Convolution과 같은 모듈도 데이터에 적합한 것을 찾는 알고리즘이 필요하다(라고 주장한다).</li>
        <li>MDL로 Fully-Connected NN에 비해 CNN이 가지는 이점을 공식화했다.</li>
        <li>Fully-Connected NN으로부터 Pruning을 통해 CNN 이상의 성능을 유도하기 위해, LASSO의 변형인 <katex-expression expression='\beta-' katex-options='{ "displayMode": false , "throwOnError": true }' />LASSO를 사용했다.</li>
    </ol>

    <h2>본문 요약</h2>

    <p>저자들은 D-Conv, D-Local, D-FC, S-Conv, S-Local, S-FC, 3-FC에 대한 성능 측정을 통해 다음과 같은 사실을 관찰한다.</p>
    <ol>
        <li>Convolution의 주요한 이점은 지역성이다.</li>
        <li>Deep NN은 긴 epoch에서 성능 증가폭이 더디거나 오히려 감소하는 반면, Shallow NN은 일관적으로 성능이 증가한다.</li>
        <li>Weight-Sharing이 없는 DNN은 성능이 저하한다.</li>
        <li>같은 파라미터 수를 가지는 모델 중 초반부 파라미터가 많은 모델의 성능이 좋다.</li>
    </ol>
    <p><a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Shalev-Shwartz and Ben-David, 2014</a>에서 정의한 Minimum Description Language <katex-expression expression='L_D\left(h\right)\le L_S\left(h\right)+\sqrt{\frac{\left\lvert d\left(h\right)\right\rvert+\log{2/\delta}}{2m}}' katex-options='{ "displayMode": false , "throwOnError": true }' />을 보면, Generalization-Gap은 모델의 파라미터 수로 통제할 수 있다. 그러나 지금까지 제시된 딥러닝 아키텍쳐들을 볼 때, 아키텍쳐를 잘 골라서 파라미터 수를 스케일하면 성능이 저하되지 않는다. 그 중에서도 적은 파라미터로 트레이닝 데이터에 피팅하는 아키텍쳐가 우월한 성능을 낸다.</p>
    <p>위 식에서 아키텍쳐 선택 과정을 고려하면 다음과 같다: <katex-expression expression='L_D\left(f_w\right)\le L_S\left(f_w\right)+\sqrt{\frac{kb+\left\lvert d\left(g\right)\right\rvert+\log{2n/\delta}}{2m}}' katex-options='{ "displayMode": false , "throwOnError": true }' /></p>
    <p>이 공식에서는 학습된 모델의 용량이 매개변수의 비트수와 Weight-Sharing의 Description Length 두 항으로 제한된다는 것을 알 수 있다. 즉 파라미터 수를 줄이고 구조적으로 Weight-Sharing을 할 때, Generalization-Gap을 줄일 수 있다.</p>
    <p>파라미터를 효율적으로 줄이기 위해, <katex-expression expression='\beta-' katex-options='{ "displayMode": false , "throwOnError": true }' />LASSO를 사용한다. 공격적으로 파라미터를 줄이기 위해 추가 하이퍼 파라미터 <katex-expression expression='\beta' katex-options='{ "displayMode": false , "throwOnError": true }' />를 사용한다. 파라미터 절대값의 최소 크기를 정해서, 영향이 적은 파라미터를 제거하는 과정이 추가되었다.</p>
    <div class="img-wrap">
        <img class="fit-img" src="assets/images/beta-lasso.png" alt="beta lasso algorithm">
    </div>
    <p><katex-expression expression='\beta-' katex-options='{ "displayMode": false , "throwOnError": true }' />LASSO를 사용한 실험에서, 픽셀의 순서를 바꿔도 학습에 문제가 없다는 특징이 발견되었다. 또한 학습된 NN이 데이터로부터 Sparse-Sampling 비슷한 필터를 학습한 것을 관찰했다.</p>

    <h2>고찰</h2>

    <h3>Gradient 폭발</h3>

    <p>Gradient 소멸을 방지하기 위해서는 <katex-expression expression='\beta' katex-options='{ "displayMode": false , "throwOnError": true }' />를 1 이상으로 두면 된다. 그런데 이렇게 하면 DNN에서 무조건 Gradient 폭발이 일어난다. 이를 해결하려면 특정 절대값에 대한 Ridge 및 출력값의 Normalization이 필요하다.</p>

    <h3>Sparsity</h3>

    <p><katex-expression expression='\beta-' katex-options='{ "displayMode": false , "throwOnError": true }' />LASSO를 적용한다고 해도 그것이 파라미터의 Sparsity를 보장하지는 않는다. 많아도 Fully-Connected 레이어 기준 30% 이하의 파라미터만이 0이 아닌 값을 가져야 Sparse 필터라고 할 수 있다고 생각한다. 따라서 해당 조건을 만족할 때까지, 파라미터를 절대값 순으로 정렬하고 하위의 것을 제거하는 과정을 추가해야 한다.</p>

    <h3>Weight-Sharing</h3>
    
    <p>Weight-Sharing을 위해서는 필터를 여러 노드가 공유해야 한다. 입력 데이터를 특정한 규칙으로 순회하면서 노드를 만들어주면 된다. Shallow NN을 만들기 위해 앞단 레이어의 필터는 크게 해야 한다.</p>
</div>