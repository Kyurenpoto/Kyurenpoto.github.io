@page "/post/2"

<MetaTags title="논문 리뷰: Towards Learning Convolutions from Scratch - " description="논문 리뷰: Towards Learning Convolutions from Scratch" route="/post/2" keywords="deep learning, activation function, skip connection, inverse problem" pathImage="assets/images/sine-curve.png" />

<Link Href="styles/post.css" Rel="stylesheet" />

<div class="content-wrap">
    <h1>논문 리뷰: Towards Learning Convolutions from Scratch</h1>
    
    <p>논문 링크: <a href="https://arxiv.org/abs/2007.13657">https://arxiv.org/abs/2007.13657</a></p>
    <p>논문의 초록, 서론, 결론을 종합하면 다음과 같다.</p>
    <ol>
        <li>최신 NN 아키텍쳐 검색 알고리즘에서는 Convolution을 기본 모듈로 사용한다. 아키텍쳐 검색 알고리즘처럼 Convolution과 같은 모듈도 데이터에 적합한 것을 찾는 알고리즘이 필요하다(라고 주장한다).</li>
        <li>MDL로 Fully-Connected NN에 비해 CNN이 가지는 이점을 공식화했다.</li>
        <li>Fully-Connected NN으로부터 Pruning을 통해 CNN 이상의 성능을 유도하기 위해, LASSO의 변형인 <katex-expression expression='\beta-' katex-options='{ "displayMode": false , "throwOnError": true }' />LASSO를 사용했다.</li>
    </ol>

    <h2>본문 요약</h2>

    <p>저자들은 D-Conv, D-Local, D-FC, S-Conv, S-Local, S-FC, 3-FC에 대한 성능 측정을 통해 다음과 같은 사실을 관찰한다.</p>
    <ol>
        <li>Convolution의 주요한 이점은 지역성이다.</li>
        <li>Deep NN은 긴 epoch에서 성능 증가폭이 더디거나 오히려 감소하는 반면, Shallow NN은 일관적으로 성능이 증가한다.</li>
        <li>Weight-Sharing이 없는 DNN은 성능이 저하한다.</li>
        <li>같은 파라미터 수를 가지는 모델 중 초반부 파라미터가 많은 모델의 성능이 좋다.</li>
    </ol>
    <p><a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Shalev-Shwartz and Ben-David, 2014</a>에서 정의한 Minimum Description Language <katex-expression expression='L_D\left(h\right)\le L_S\left(h\right)+\sqrt{\frac{\left\lvert d\left(h\right)\right\rvert+\log{2/\delta}}{2m}}' katex-options='{ "displayMode": false , "throwOnError": true }' />을 보면, Generalization-Gap은 모델의 파라미터 수로 통제할 수 있다. 그러나 지금까지 제시된 딥러닝 아키텍쳐들을 볼 때, 아키텍쳐를 잘 골라서 파라미터 수를 스케일하면 성능이 저하되지 않는다. 그 중에서도 적은 파라미터로 트레이닝 데이터에 피팅하는 아키텍쳐가 우월한 성능을 낸다.</p>
    <p>위 식에서 아키텍쳐 선택 과정을 고려하면 다음과 같다: <katex-expression expression='L_D\left(f_w\right)\le L_S\left(f_w\right)+\sqrt{\frac{kb+\left\lvert d\left(g\right)\right\rvert+\log{2n/\delta}}{2m}}' katex-options='{ "displayMode": false , "throwOnError": true }' /></p>
    <p>이 공식에서는 학습된 모델의 용량이 매개변수의 비트수와 Weight-Sharing의 Description Length 두 항으로 제한된다는 것을 알 수 있다. Weight-Sharing이 구조화될 때 더 효율적으로 인코딩되며, 이 때 Generalization은 최종 모델의 파라미터 수에만 의존한다. 즉 파라미터 수를 줄이고 구조적으로 Weight-Sharing을 할 때, Generalization-Gap을 줄일 수 있다.</p>
    <p>파라미터를 효율적으로 줄이기 위해, <katex-expression expression='\beta-' katex-options='{ "displayMode": false , "throwOnError": true }' />LASSO를 사용한다.</p>
    <div class="img-wrap">
        <img class="fit-img" src="assets/images/beta-lasso.png" alt="beta lasso algorithm">
    </div>
    <p><katex-expression expression='\beta-' katex-options='{ "displayMode": false , "throwOnError": true }' />LASSO는 공격적으로 파라미터를 줄이기 위해 추가 하이퍼 파라미터 <katex-expression expression='\beta' katex-options='{ "displayMode": false , "throwOnError": true }' />를 사용한다. 파라미터 절대값의 최소 크기를 정해서, 영향이 적은 파라미터를 제거하는 과정이 추가되었다.</p>
    <p><katex-expression expression='\beta-' katex-options='{ "displayMode": false , "throwOnError": true }' />LASSO를 사용한 실험에서, 데이터의 순서를 바꿔도 학습에 문제가 없다는 특징이 발견되었다. 또한 학습된 NN이 데이터로부터 Sparse-Sampling 비슷한 필터를 학습한 것을 관찰했다.</p>

    <h2>고찰</h2>

    <h3>Gradient 폭발과 비효율성</h3>

    <p><katex-expression expression='\beta-' katex-options='{ "displayMode": false , "throwOnError": true }' />LASSO의 문제 중 하나는 Gradient 소멸은 어느 정도 방지할 수 있지만 Gradient 폭발은 그렇지 않다는 것이다.</p>
    <p>Gradient 소멸을 방지하기 위해서는 <katex-expression expression='\beta' katex-options='{ "displayMode": false , "throwOnError": true }' />를 1 이상으로 두면 된다. 그런데 이렇게 하면 DNN에서 무조건 Gradient 폭발이 일어난다. 이를 해결하려면 파라미터의 Clamp 및 각 레이어의 출력 값의 Normalization이 필요하다. 그러면 안 그래도 Fully-Connected NN으로 시작해서 시간과 메모리를 많이 쓰는데, 추가로 더 소비하게 된다.</p>
    <p>결국 시간과 메모리를 매우 많이 사용하는 비효율적인 학습을 하게 된다.</p>

    <h3>비효율성의 극복</h3>

    <p>LASSO 방식을 사용해야 데이터에 적합한 모양의 필터를 학습할 수 있다. 그렇지만 아키텍쳐의 모든 레이어를 Fully-Connected 레이어로 설정한 후 시작하는 것은 너무나도 비효율적이다.</p>
    <p>초반 레이어의 필터를 잘 학습해야 좋은 Representation을 만들 수 있으므로, 초반 레이어부터 Fully-Connected 레이어로 설정해 학습하는 것이 좋다. 레이어 수를 점점 늘려가면서 학습하며, 필터의 모양을 학습한 레이어는 모양이 변경되지 않도록 값을 고정한다. 초반 레이어에 파라미터를 많이 두고 필터가 Sparse해야 하므로, Bottleneck 구조를 활용해 다수의 Sparse한 필터를 사용하는 구조를 기본으로 한다. 필요에 따라 Skip-Connection을 추가한다.</p>
</div>