@page "/post/1"

<MetaTags title="논문 리뷰: Implicit Neural Representations with Periodic Activation Functions - " description="논문 리뷰: Implicit Neural Representations with Periodic Activation Functions" route="/post/1" keywords="deep learning, activation function, skip connection, inverse problem" pathImage="assets/images/sine-curve.png" />

<Link Href="styles/post.css" Rel="stylesheet" />

<h1>논문 리뷰: Implicit Neural Representations with Periodic Activation Functions</h1>

<p>논문 링크: <a href="https://arxiv.org/abs/2006.09661">https://arxiv.org/abs/2006.09661</a></p>
<p>논문의 초록, 서론, 결론을 종합하면 다음과 같다.</p>
<ol>
    <li>기존의 ReLU 기반 MLP는 입력 신호의 고차 미분을 나타낼 수 없기 때문에 Micro Representaion 능력이 부족하다.</li>
    <li>이를 개선하기 위해 주기 함수를 활성 함수로 사용해서 고차 미분 정보를 회손하지 않도록 했다.</li>
    <li>다양한 문제에서 기존 모델과 비교했을 때 더 나은 Representaion을 보임을 실험을 통해 보여준다. 논문에서는 Activation Function을 바꿔주고 적절하게 초기화하는 것만으로도 성능이 올라간다고 주장한다.</li>
</ol>
<p>이 글에서는 논문의 요점을 간략하게 요약하고, 이에 관련된 여러 테마에 대한 몇 가지 고찰을 내 방식대로 진행한다. 자세한 수식 유도 및 증명은 논문을 참고하면 된다.</p>

<h2>SIREN의 Activation Function 및 초기화</h2>

<img src="assets/images/sine-curve.png" alt="sine curve">

<p>SIREN에서 사용한 Activation Function은 <katex-expression expression='\sin' katex-options='{ "displayMode": false , "throwOnError": true }' /> 함수이다. 단순히 Activation Function에 <katex-expression expression='\sin' katex-options='{ "displayMode": false , "throwOnError": true }' /> 함수를 사용한 것이다.</p>
<p>논문에서는 SIREN의 모든 가중치의 각 성분이 <katex-expression expression='w_i\text{\textasciitilde}U\left(-6/\sqrt{n},6/\sqrt{n}\right)\quad\left(c\in\mathbb{R}\right)' katex-options='{ "displayMode": false , "throwOnError": true }' /> 로 분포하도록 초기화하도록 제안한다.</p>

<h2>고찰</h2>

<h3>Weight/Bias 정규화</h3>

<p><katex-expression expression='\sin' katex-options='{ "displayMode": false , "throwOnError": true }' /> 을 사용하면 Gradient가 증발하거나 폭발하는 문제가 거의 없다. 입력과 Weight의 내적의 각 원소에 대한 <katex-expression expression='\sin' katex-options='{ "displayMode": false , "throwOnError": true }' /> 값이 <katex-expression expression='\left(-\pi,\pi\right)' katex-options='{ "displayMode": false , "throwOnError": true }' /> 를 거의 벗어나지 않는 표준 정규 분포를 이루기 때문이다. 저 범위를 벗어나는 경우 Bias를 조작해서 저 범위 안쪽으로 끌고 오면 그만이다.</p>
<p>어차피 두 번째 레이어부터는 Graident 문제가 없으므로, 첫 레이어에서 발생하는 문제만 고려하면 된다. 첫 레이어에 들어오는 입력, 즉 모델의 입력은 그 성질에 따라 값의 범위와 분포가 제각각이다. 이를 강제로 맞춰주기 위해서는 Weight Standardization을 사용하면 된다.</p>

<h3>Skip Connection의 대체</h3>

<p>기존의 방법론에서는 Activation Function들이 가지는 Gradient 관련 문제 때문에 Skip Connection이 도입되었다. 앞 단계 레이어의 출력을 가져와서 사용하면 Gradient 문제를 완화하는 동시에 해당 정보를 뒷 단계의 레이어에서 활용할 수 있다.</p>
<p>그러나 Residual Block에 포함된 레이어가 학습한 Feature만을 사용할 수 있고, 레이어의 출력이 Identity와 같아야만 한다는 문제가 있다.</p>
<p>SIREN에서는 Residual Block과는 관계 없이 이전 레이어가 학습한 Feature를 사용할 수 있다. 레이어의 출력 형태에 제한을 두지 않는다는 장점도 있다.</p>

<h3>Inverse Problem에서의 활용</h3>

<p>SIREN은 미분하면 SIREN이 나오는 특성이 있다. 논문에서는 이 특성과 Neural ODE를 이용해 Inverse Problem을 푸는데 사용할 수 있을 것이라고 언급하고 있다.</p>
<p>Inverse Problem은 관계 <katex-expression expression='R=\left\lbrace\left(x,y\right)\in X\times Y\right\rbrace' katex-options='{ "displayMode": false , "throwOnError": true }' /> 를 만족하는 <katex-expression expression='y\in Y' katex-options='{ "displayMode": false , "throwOnError": true }' /> 로부터 <katex-expression expression='x\in X' katex-options='{ "displayMode": false , "throwOnError": true }' /> 를 추정하는 문제이다. 주로 관찰한 결과를 이용해 기존 모델에서 사용되는 데이터를 추정하는데 사용된다.</p>
<p>딥러닝으로 이 문제를 해결할 때는 관계/역관계를 Neural Network로 근사하고, 이를 인코더-디코더 네트워크 구조로 구성하는 방법을 사용한다. 여기에 SIREN + Neural ODE를 적용하면 Representation 품질의 향상과 더불어, Neural ODE의 역전파를 사용하지 않는 특성 덕분에 Convolution 계산에 FFT를 쓸 수 있게 된다. SIREN의 미분에 관한 특성 및 <katex-expression expression='\sin' katex-options='{ "displayMode": false , "throwOnError": true }' /> 함수의 잘 알려진 특성을 활용하면 계산의 정확도 향상도 이룰 수 있다.</p>
