@page "/post/1"

<MetaTags title="논문 리뷰: Implicit Neural Representations with Periodic Activation Functions - " description="논문 리뷰: Implicit Neural Representations with Periodic Activation Functions" route="/post/1" keywords="deep learning, activation function, skip connection, inverse problem" pathImage="assets/images/sine-curve.png" />

<Link Href="styles/post.css" Rel="stylesheet" />

<h1>논문 리뷰: Implicit Neural Representations with Periodic Activation Functions</h1>

<p>논문 링크: <a href="https://arxiv.org/abs/2006.09661">https://arxiv.org/abs/2006.09661</a></p>
<p>논문의 초록, 서론, 결론을 종합하면 다음과 같다.</p>
<ol>
    <li>기존의 ReLU 기반 MLP는 입력 신호의 고차 도함수 정보가 망가지기 때문에 Micro Representaion 능력이 부족하다.</li>
    <li>이를 개선하기 위해 주기 함수를 활성 함수로 사용해서 고차 도함수 정보를 회손하지 않도록 했다.</li>
    <li>다양한 문제에서 기존 모델과 비교했을 때 더 나은 Representaion을 보임을 실험을 통해 보여준다. 논문에서는 Activation Function을 바꿔주고 적절하게 초기화하는 것만으로도 성능이 올라간다고 주장한다.</li>
</ol>
<p>이 글에서는 논문의 요점을 간략하게 요약하고, 이에 관련된 여러 테마에 대한 몇 가지 고찰을 내 방식대로 진행한다. 자세한 수식 유도 및 증명은 논문을 참고하면 된다.</p>

<h2>SIREN의 Activation Function 및 초기화</h2>

<img src="assets/images/sine-curve.png" alt="sine curve">

<p>SIREN에서 사용한 Activation Function은 <katex-expression expression='\sin' katex-options='{ "displayMode": false , "throwOnError": true }' /> 함수이다. 단순히 Activation Function에 <katex-expression expression='\sin' katex-options='{ "displayMode": false , "throwOnError": true }' /> 함수를 사용한 것이다.</p>
<p>각 레이어에 대해 Gradient를 계산하면 선형 함수와 비선형 함수를 번갈아가며 합성하는 꼴이 나온다. <katex-expression expression='\sin' katex-options='{ "displayMode": false , "throwOnError": true }' /> 함수는 미분하게 되면 다시 <katex-expression expression='\sin' katex-options='{ "displayMode": false , "throwOnError": true }' /> 함수가 되고, 선형 함수는 미분하면 Transpose한 꼴의 행렬이 된다. 따라서 각 레이어에 대한 Gradient는 또 다른 SIREN이 된다. 입력을 <katex-expression expression='\left(-1,1\right)' katex-options='{ "displayMode": false , "throwOnError": true }' /> 범위의 균등 분포로 가정할 때 SIREN의 출력은 표준 정규 분포를 따르므로, SIREN의 Gradient는 폭발/소멸에서 안전하다고 볼 수 있다.</p>
<p>논문에서는 SIREN의 모든 가중치의 각 성분이 <katex-expression expression='w_i\text{\textasciitilde}U\left(-6/\sqrt{n},6/\sqrt{n}\right)\quad\left(c\in\mathbb{R}\right)' katex-options='{ "displayMode": false , "throwOnError": true }' /> 로 분포하도록 초기화하도록 제안한다.</p>
<p>마지막으로 SIREN의 특성을 이용해 Neural ODE와 같은 Inverse Problem을 Robust하게 풀 수 있을 것으로 예상하며, 이를 Future Work로 남겨두고 있다.</p>

<h2>고찰</h2>

<h3>Weight/Bias Normalization</h3>

<p>위에서 언급한 것처럼, <katex-expression expression='\sin' katex-options='{ "displayMode": false , "throwOnError": true }' /> 을 사용하면 Gradient가 증발하거나 폭발하는 문제가 거의 없다. 남은건 Weight/Bias의 Normliazation인데, 따로 WN을 적용할 필요는 없고 <katex-expression expression='\sin' katex-options='{ "displayMode": false , "throwOnError": true }' /> 함수의 성질을 이용해 0에 가장 가까운 값으로 바꿔주면 된다.</p>
<p>어차피 두 번째 레이어부터는 Graident 문제가 없으므로, 첫 레이어에서 발생하는 문제만 고려하면 된다. 첫 레이어에 들어오는 입력, 즉 모델의 입력은 그 성질에 따라 값의 범위와 분포가 제각각이다. 이를 강제로 맞춰주기 위해서는 첫 레이어에만 WN을 적용하면 된다.</p>

<h3>Skip Connection의 대체</h3>

<p>기존의 방법론에서는 Activation Function들이 가지는 Gradient 관련 문제 때문에 Skip Connection이 도입되었다. 앞 단계 레이어의 출력을 가져와서 사용하면 Gradient 문제를 완화하는 동시에 해당 정보를 뒷 단계의 레이어에서 활용할 수 있다.</p>
<p>그러나 자신이 포함된 Residual Block의 Feature만을 활용할 수 있고, 레이어의 출력이 Identity와 같아야만 한다는 문제가 있다.</p>
<p>SIREN에서는 고차 도함수 정보가 망가지지 않기 때문에, Residual Block과는 관계 없이 이전 레이어가 학습한 Feature를 활용할 수 있다. 레이어의 출력 형태에 제한을 두지 않는다는 장점도 있다.</p>
