@page "/post/1"

<MetaTags title="논문 리뷰: Implicit Neural Representations with Periodic Activation Functions - " description="논문 리뷰: Implicit Neural Representations with Periodic Activation Functions" route="/post/1" />

<h1>논문 리뷰: Implicit Neural Representations with Periodic Activation Functions</h1>

<p>논문 링크: <a href="https://arxiv.org/abs/2006.09661">https://arxiv.org/abs/2006.09661</a></p>
<p>논문의 초록, 서론, 결론을 종합하면 다음과 같다.</p>
<ol>
    <li>기존의 ReLU 기반 MLP는 입력 신호의 고차 미분을 나타낼 수 없기 때문에 미세 표현 능력이 부족하다.</li>
    <li>이를 개선하기 위해 주기 함수를 활성 함수로 사용해서 고차 미분 정보를 회손하지 않도록 했다.</li>
    <li>다양한 문제에서 기존 모델과 비교했을 때 더 나은 표현을 보임을 실험을 통해 증명했다.</li>
</ol>
<p>이 글에서는 논문의 요점을 간략하게 요약하고, 이에 관련된 여러 테마에 대한 몇 가지 고찰을 내 방식대로 진행한다. 자세한 수식 유도 및 증명은 논문을 참고하면 된다.</p>

<hr />

<h2>요점 요약</h2>

<h3>문제 설정</h3>

<p>우리가 풀어야 하는 문제는 다음과 같은 미분방정식으로 표현할 수 있다.</p>
<p class="katex">
    <katex-expression expression='F\left(\mathbf{x},\Phi\left(\mathbf{x}\right),\nabla\Phi\left(\mathbf{x}\right),\cdots\right)=0' katex-options='{ "displayMode": true , "throwOnError": true }' />
</p>
<p>이걸 <katex-expression expression='M' katex-options='{ "displayMode": false , "throwOnError": true }' /> 개의 제약 조건에 대한 식으로 다음과 같이 바꿀 수 있다.</p>
<p class="katex">
    <katex-expression expression='C_m\left(\mathbf{a}\left(\mathbf{x}\right),\Phi\left(\mathbf{x}\right),\nabla\Phi\left(\mathbf{x}\right),\cdots\right)=0\qquad\left(\forall\mathbf{x}\in\Omega_m\quad m\in\left\lbrace 1,\cdots\,M\right\rbrace\right)' katex-options='{ "displayMode": true , "throwOnError": true }' />
</p>
<p>이 식은 다시 손실 함수로 변환할 수 있다.</p>
<p class="katex">
    <katex-expression expression='L=\int_{\Omega}\sum_{m=1}^{M}1_{\Omega_m}\left(\mathbf{x}\right)\left\|C_m\left(\mathbf{a}\left(\mathbf{x}\right),\Phi\left(\mathbf{x}\right),\nabla\Phi\left(\mathbf{x}\right),\cdots\right)\right\|d\mathbf{x}\qquad\left(1_{\Omega_m}\left(\mathbf{x}\right)=\begin{cases}1 & \mathbf{x}\in\Omega_m\\0 & \mathbf{x}\notin\Omega_m\end{cases}\right)' katex-options='{ "displayMode": true , "throwOnError": true }' />
</p>
<p>손실 함수는 <katex-expression expression='\Phi\left(\mathbf{x}\right)' katex-options='{ "displayMode": false , "throwOnError": true }' /> 의 정의역에서 샘플링해서 추정한다. 데이터 셋은 실행 중 동적으로 추출하며, <katex-expression expression='\left\lbrace\left(\mathbf{x}_i,\mathbf{a}_i\left(\mathbf{x}\right)\right)\right\rbrace_i\;\left(\mathbf{x}_i\in\Omega\right)' katex-options='{ "displayMode": false , "throwOnError": true }' /> 의 형태로 주어진다.</p>
<p>따라서 Loss는 다음과 같다.</p>
<p class="katex">
    <katex-expression expression='\bar{L}=\sum_{i\in D}\sum_{m=1}^{M}\left\|C_m\left(a\left(\mathbf{x}_i\right),\Phi\left(\mathbf{x}_i\right),\nabla\Phi\left(\mathbf{x}_i\right),\cdots\right)\right\|' katex-options='{ "displayMode": true , "throwOnError": true }' />
</p>

<h3>SIREN의 Activation Function 및 초기화</h3>

<p>SIREN에서 사용한 Activation Function은 <katex-expression expression='\sin' katex-options='{ "displayMode": false , "throwOnError": true }' /> 함수이다. 단순히 Activation Function 에 <katex-expression expression='\sin' katex-options='{ "displayMode": false , "throwOnError": true }' /> 함수를 사용한 것이다.</p>
<p>논문에서는 SIREN의 모든 가중치의 각 성분이 <katex-expression expression='w_i\text{\textasciitilde}U\left(-6/\sqrt{n},6/\sqrt{n}\right)\quad\left(c\in\mathbb{R}\right)' katex-options='{ "displayMode": false , "throwOnError": true }' /> 로 분포하도록 초기화하도록 제안한다.</p>

<hr />

<h2>고찰</h2>

<h3>weight/bias 정규화</h3>

<p>사인 함수를 사용하면 gradiant 가 증발하거나 폭발하는 문제가 거의 없다. 사인 함수의 치역에 특정 균등 분포에서 추출한 값을 곱한 값이 (-pi, pi)를 거의 벗어나지 않는 표준 정규 분포를 이루기 때문이다. 저 범위를 벗어나는 경우 그래디언트를 조작해서 저 범위 안쪽으로 끌고 오면 그만이다.</p>
<p>하지만 이렇게 해도 activation function 을 사인 함수로 통일할 수는 없다. 입력값의 범위가 알려진 경우 첫 레이어에서 weight/bias 정규화를 사용하면 되지만, 정규화가 불가능한 경우 모델이 상정하는 입력값 범위 밖의 outlier 에 대한 처리가 애매하다.</p>
<p>따라서 주기성과 선형성을 가지지 않는 연속인 비선형 일대일 대응 함수(Sigmoid, tanh)가 필요하다. 앞 부분에는 Sigmoid 또는 tanh를 적용하고 그 뒤부터는 sin을 사용하면 대부분의 문제가 해결될 것이다.</p>

<h3>Skip Connection 과의 관계</h3>

<p>기존의 방법론에서는 Activation Function 들이 가지는 gradiant 관련 문제 때문에 Skip Connection 이 도입되었다. 앞 단계의 레이어에서 output을 가져와서 사용하면 gradiant 문제를 완화하는 동시에 해당 gradiant 정보를 뒷 단계의 레이어에서 활용할 수 있다.</p>
<p>다만 Skip Conection 은 서로 연결할 레이어들의 output shape 를 강제하며, 추가적인 연산 비용이 든다. 이는 더 큰 모델이 나올수록 기하급수적으로 많은 계산 성능이 필요한 원인이 된다.</p>
<p>SIREN 을 이용하면 앞 단계의 레이어를 연결하지 않고 고차 gradiant 를 사용할 수 있다. 이 비용은 Skip Connection에 비해 훨씬 작다.</p>

<h3>Neural ODE 와의 연계</h3>

<p>SIREN 은 미분하면 SIREN이 나오는 특성이 있다. 논문에서는 이를 이용해 Neural ODE 와 같은 Inverse Problem 를 푸는데 사용할 수 있을 것이라고 언급하고 있다.</p>

<p>작성중..</p>
